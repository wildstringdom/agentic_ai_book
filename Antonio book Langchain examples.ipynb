{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"10WrM1Vr2U2AG4nkESx38tKSYyn06jlIa","timestamp":1760574153831}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ACaluuCD0Z4","executionInfo":{"status":"ok","timestamp":1760478712870,"user_tz":420,"elapsed":18939,"user":{"displayName":"umut ozertem","userId":"07706305058932434778"}},"outputId":"8075af1f-0d17-41b8-f3a9-605df6b000c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n","Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.31)\n","Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.35)\n","Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (0.6.10)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.78)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n","Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.33)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.0)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n","Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n","Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.2)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n","Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n","Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n","Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.1.2)\n","Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.6.4)\n","Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.2.9)\n","Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n","Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n","Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n","Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n","Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.11.0)\n","Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n","Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.11.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.67.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.10.5)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n","Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (2.1.12)\n","Requirement already satisfied: langchain-core>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.3.78)\n","Requirement already satisfied: google-ai-generativelanguage<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.7.0)\n","Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.11.10)\n","Requirement already satisfied: filetype<2,>=1.2 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.0)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.25.2)\n","Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.38.0)\n","Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.26.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.29.5)\n","Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (0.4.33)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (1.33)\n","Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (6.0.3)\n","Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (4.15.0)\n","Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (25.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.2)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.70.0)\n","Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.32.5)\n","Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.75.1)\n","Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.71.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (4.9.1)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.3.75->langchain-google-genai) (3.0.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.28.1)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (3.11.3)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.25.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (4.11.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (3.10)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.16.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.6.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (3.4.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.5.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.3.1)\n","Requirement already satisfied: langchain_openai in /usr/local/lib/python3.12/dist-packages (0.3.35)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.78 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.3.78)\n","Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.109.1)\n","Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n","Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.4.33)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (1.33)\n","Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (6.0.3)\n","Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (4.15.0)\n","Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (25.0)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.78->langchain_openai) (2.11.10)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (0.11.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain_openai) (4.67.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.32.5)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.104.2->langchain_openai) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.104.2->langchain_openai) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.104.2->langchain_openai) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.104.2->langchain_openai) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.78->langchain_openai) (3.0.0)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (3.11.3)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.25.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.78->langchain_openai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.78->langchain_openai) (0.4.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (3.4.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.5.0)\n"]}],"source":["!pip install langchain langchain-community langchain-openai langgraph\n","!pip install langchain-google-genai\n","!pip install langchain_openai\n","\n"]},{"cell_type":"markdown","source":["## 1. Chaining"],"metadata":{"id":"3YQr77hmtZ1X"}},{"cell_type":"code","source":["import os\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# For better security, load environment variables from a .env file\n","import os\n","\n","# TODO(umut) -- keeping this in the colab for now while we are both running over\n","# the examples. clean up before we share\n","os.environ['OPENAI_API_KEY'] = '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'\n","print(os.environ.get('MY_VARIABLE'))\n","\n","# Make sure your OPENAI_API_KEY is set in the .env file\n","# Initialize the Language Model (using ChatOpenAI is recommended)\n","llm = ChatOpenAI(temperature=0)\n","\n","# --- Prompt 1: Extract Information ---\n","prompt_extract = ChatPromptTemplate.from_template(\n","   \"Extract the technical specifications from the following text:\\n\\n{text_input}\"\n",")\n","\n","# --- Prompt 2: Transform to JSON ---\n","prompt_transform = ChatPromptTemplate.from_template(\n","   \"Transform the following specifications into a JSON object with 'cpu', 'memory', and 'storage' as keys:\\n\\n{specifications}\"\n",")\n","\n","# --- Build the Chain using LCEL ---\n","# The StrOutputParser() converts the LLM's message output to a simple string.\n","extraction_chain = prompt_extract | llm | StrOutputParser()\n","\n","# The full chain passes the output of the extraction chain into the 'specifications'\n","# variable for the transformation prompt.\n","full_chain = (\n","   {\"specifications\": extraction_chain}\n","   | prompt_transform\n","   | llm\n","   | StrOutputParser()\n",")\n","\n","# --- Run the Chain ---\n","input_text = \"The new laptop model features a 3.5 GHz octa-core processor, 16GB of RAM, and a 1TB NVMe SSD.\"\n","\n","# Execute the chain with the input text dictionary.\n","final_result = full_chain.invoke({\"text_input\": input_text})\n","\n","print(\"\\n--- Final JSON Output ---\")\n","print(final_result)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DnapFHLUD9IT","executionInfo":{"status":"ok","timestamp":1760478393766,"user_tz":420,"elapsed":24390,"user":{"displayName":"umut ozertem","userId":"07706305058932434778"}},"outputId":"7cc9d521-f54e-4302-c124-5f6e1ee9f0c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["None\n","\n","--- Final JSON Output ---\n","{\n","    \"cpu\": {\n","        \"Processor\": \"3.5 GHz octa-core\"\n","    },\n","    \"memory\": {\n","        \"RAM\": \"16GB\"\n","    },\n","    \"storage\": {\n","        \"Storage\": \"1TB NVMe SSD\"\n","    }\n","}\n"]}]},{"cell_type":"markdown","source":["## 2. Routing"],"metadata":{"id":"GZaRrLJAtelx"}},{"cell_type":"code","source":["from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough, RunnableBranch\n","\n","import os\n","\n","# TODO(umuto): clean up and put into env\n","os.environ['GOOGLE_API_KEY'] = 'AIzaSyBjEJoyoFZ1buC2NGap1PyThe3reHK9u-o'\n","\n","# --- Configuration ---\n","# Ensure your API key environment variable is set (e.g., GOOGLE_API_KEY)\n","try:\n","    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n","    print(f\"Language model initialized: {llm.model}\")\n","except Exception as e:\n","    print(f\"Error initializing language model: {e}\")\n","    llm = None\n","\n","# --- Define Simulated Sub-Agent Handlers (equivalent to ADK sub_agents) ---\n","\n","def booking_handler(request: str) -> str:\n","    \"\"\"Simulates the Booking Agent handling a request.\"\"\"\n","    print(\"\\n--- DELEGATING TO BOOKING HANDLER ---\")\n","    return f\"Booking Handler processed request: '{request}'. Result: Simulated booking action.\"\n","\n","def info_handler(request: str) -> str:\n","    \"\"\"Simulates the Info Agent handling a request.\"\"\"\n","    print(\"\\n--- DELEGATING TO INFO HANDLER ---\")\n","    return f\"Info Handler processed request: '{request}'. Result: Simulated information retrieval.\"\n","\n","def unclear_handler(request: str) -> str:\n","    \"\"\"Handles requests that couldn't be delegated.\"\"\"\n","    print(\"\\n--- HANDLING UNCLEAR REQUEST ---\")\n","    return f\"Coordinator could not delegate request: '{request}'. Please clarify.\"\n","\n","# --- Define Coordinator Router Chain (equivalent to ADK coordinator's instruction) ---\n","# This chain decides which handler to delegate to.\n","coordinator_router_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"\"\"Analyze the user's request and determine which specialist handler should process it.\n","     - If the request is related to booking flights or hotels, output 'booker'.\n","     - For all other general information questions, output 'info'.\n","     - If the request is unclear or doesn't fit either category, output 'unclear'.\n","     ONLY output one word: 'booker', 'info', or 'unclear'.\"\"\"),\n","    (\"user\", \"{request}\")\n","])\n","\n","if llm:\n","    coordinator_router_chain = coordinator_router_prompt | llm | StrOutputParser()\n","\n","# --- Define the Delegation Logic (equivalent to ADK's Auto-Flow based on sub_agents) ---\n","# Use RunnableBranch to route based on the router chain's output.\n","\n","# Define the branches for the RunnableBranch\n","branches = {\n","    \"booker\": RunnablePassthrough.assign(output=lambda x: booking_handler(x['request']['request'])),\n","    \"info\": RunnablePassthrough.assign(output=lambda x: info_handler(x['request']['request'])),\n","    \"unclear\": RunnablePassthrough.assign(output=lambda x: unclear_handler(x['request']['request'])),\n","}\n","\n","# Create the RunnableBranch. It takes the output of the router chain\n","# and routes the original input ('request') to the corresponding handler.\n","delegation_branch = RunnableBranch(\n","    (lambda x: x['decision'].strip() == 'booker', branches[\"booker\"]), # Added .strip()\n","    (lambda x: x['decision'].strip() == 'info', branches[\"info\"]),     # Added .strip()\n","    branches[\"unclear\"] # Default branch for 'unclear' or any other output\n",")\n","\n","# Combine the router chain and the delegation branch into a single runnable\n","# The router chain's output ('decision') is passed along with the original input ('request')\n","# to the delegation_branch.\n","coordinator_agent = {\n","    \"decision\": coordinator_router_chain,\n","    \"request\": RunnablePassthrough()\n","} | delegation_branch | (lambda x: x['output']) # Extract the final output\n","\n","# --- Example Usage ---\n","def main():\n","    if not llm:\n","        print(\"\\nSkipping execution due to LLM initialization failure.\")\n","        return\n","\n","    print(\"--- Running with a booking request ---\")\n","    request_a = \"Book me a flight to London.\"\n","    result_a = coordinator_agent.invoke({\"request\": request_a})\n","    print(f\"Final Result A: {result_a}\")\n","\n","    print(\"\\n--- Running with an info request ---\")\n","    request_b = \"What is the capital of Italy?\"\n","    result_b = coordinator_agent.invoke({\"request\": request_b})\n","    print(f\"Final Result B: {result_b}\")\n","\n","    print(\"\\n--- Running with an unclear request ---\")\n","    request_c = \"uiuoiupiu\"\n","    result_c = coordinator_agent.invoke({\"request\": request_c})\n","    print(f\"Final Result C: {result_c}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BOxoKknLYNvM","executionInfo":{"status":"ok","timestamp":1760478675163,"user_tz":420,"elapsed":4051,"user":{"displayName":"umut ozertem","userId":"07706305058932434778"}},"outputId":"5b8ce927-15c5-4c62-8c65-6309fbab92f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Language model initialized: models/gemini-2.5-flash\n","--- Running with a booking request ---\n","\n","--- DELEGATING TO BOOKING HANDLER ---\n","Final Result A: Booking Handler processed request: 'Book me a flight to London.'. Result: Simulated booking action.\n","\n","--- Running with an info request ---\n","\n","--- DELEGATING TO INFO HANDLER ---\n","Final Result B: Info Handler processed request: 'What is the capital of Italy?'. Result: Simulated information retrieval.\n","\n","--- Running with an unclear request ---\n","\n","--- HANDLING UNCLEAR REQUEST ---\n","Final Result C: Coordinator could not delegate request: 'uiuoiupiu'. Please clarify.\n"]}]},{"cell_type":"markdown","source":["## 3. Parallelization"],"metadata":{"id":"cd6WprmntkeT"}},{"cell_type":"code","source":["import os\n","import asyncio\n","from typing import Optional\n","\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import Runnable, RunnableParallel, RunnablePassthrough\n","\n","import nest_asyncio\n","nest_asyncio.apply()\n","\n","\n","import os\n","\n","os.environ['OPENAI_API_KEY'] = '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'\n","\n","# --- Configuration ---\n","# Ensure your API key environment variable is set (e.g., OPENAI_API_KEY)\n","try:\n","    llm: Optional[ChatOpenAI] = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n","    if llm:\n","        print(f\"Language model initialized: {llm.model_name}\")\n","except Exception as e:\n","    print(f\"Error initializing language model: {e}\")\n","    llm = None\n","\n","\n","# --- Define Independent Chains ---\n","# These three chains represent distinct tasks that can be executed in parallel.\n","\n","summarize_chain: Runnable = (\n","    ChatPromptTemplate.from_messages([\n","        (\"system\", \"Summarize the following topic concisely:\"),\n","        (\"user\", \"{topic}\")\n","    ])\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","questions_chain: Runnable = (\n","    ChatPromptTemplate.from_messages([\n","        (\"system\", \"Generate three interesting questions about the following topic:\"),\n","        (\"user\", \"{topic}\")\n","    ])\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","terms_chain: Runnable = (\n","    ChatPromptTemplate.from_messages([\n","        (\"system\", \"Identify 5-10 key terms from the following topic, separated by commas:\"),\n","        (\"user\", \"{topic}\")\n","    ])\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","\n","# --- Build the Parallel + Synthesis Chain ---\n","\n","# 1. Define the block of tasks to run in parallel. The results of these,\n","#    along with the original topic, will be fed into the next step.\n","map_chain = RunnableParallel(\n","    {\n","        \"summary\": summarize_chain,\n","        \"questions\": questions_chain,\n","        \"key_terms\": terms_chain,\n","        \"topic\": RunnablePassthrough(),  # Pass the original topic through\n","    }\n",")\n","\n","# 2. Define the final synthesis prompt which will combine the parallel results.\n","synthesis_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"\"\"Based on the following information:\n","     Summary: {summary}\n","     Related Questions: {questions}\n","     Key Terms: {key_terms}\n","     Synthesize a comprehensive answer.\"\"\"),\n","    (\"user\", \"Original topic: {topic}\")\n","])\n","\n","# 3. Construct the full chain by piping the parallel results directly\n","#    into the synthesis prompt, followed by the LLM and output parser.\n","full_parallel_chain = map_chain | synthesis_prompt | llm | StrOutputParser()\n","\n","\n","# --- Run the Chain ---\n","async def run_parallel_example(topic: str) -> None:\n","    \"\"\"\n","    Asynchronously invokes the parallel processing chain with a specific topic\n","    and prints the synthesized result.\n","\n","    Args:\n","        topic: The input topic to be processed by the LangChain chains.\n","    \"\"\"\n","    if not llm:\n","        print(\"LLM not initialized. Cannot run example.\")\n","        return\n","\n","    print(f\"\\n--- Running Parallel LangChain Example for Topic: '{topic}' ---\")\n","    try:\n","        # The input to `ainvoke` is the single 'topic' string, which is\n","        # then passed to each runnable in the `map_chain`.\n","        response = await full_parallel_chain.ainvoke(topic)\n","        print(\"\\n--- Final Response ---\")\n","        print(response)\n","    except Exception as e:\n","        print(f\"\\nAn error occurred during chain execution: {e}\")\n","\n","if __name__ == \"__main__\":\n","    test_topic = \"The history of turkish soccer\"\n","    # In environments like Colab where an event loop is already running,\n","    # use get_event_loop() and run_until_complete() instead of asyncio.run()\n","\n","    asyncio.run(run_parallel_example(test_topic))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yA74iSXXh0u9","executionInfo":{"status":"ok","timestamp":1760478813378,"user_tz":420,"elapsed":17925,"user":{"displayName":"umut ozertem","userId":"07706305058932434778"}},"outputId":"a0f2cfbb-a3ec-476b-c2bc-177de8cadc96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Language model initialized: gpt-4o-mini\n","\n","--- Running Parallel LangChain Example for Topic: 'The history of turkish soccer' ---\n","\n","--- Final Response ---\n","The history of Turkish soccer (football) is a fascinating journey that intertwines with the cultural identity and social fabric of the nation. The sport's roots in Turkey trace back to the late 19th century, primarily influenced by the various social and cultural exchanges occurring in the region. The establishment of prominent clubs such as Galatasaray in 1905 and Fenerbahçe in 1907 marked the formal inception of organized football in Turkey. These clubs not only provided a platform for competitive sports but also played significant roles in fostering national pride and cultural identity.\n","\n","The Turkish Football Federation (TFF), founded in 1923, was a pivotal moment in the development of soccer in Turkey. This establishment allowed for the formal organization and regulation of the sport, leading to the structuring of national leagues and competitions. In its early years, the TFF oversaw the growth of local leagues and facilitated Turkey’s entry into international competitions, becoming a member of FIFA and UEFA in the same year. These milestones helped lay the groundwork for the national team and domestic leagues, ultimately leading to the establishment of the Süper Lig, Turkey's top professional league, which began in 1959. \n","\n","The cultural significance of soccer clubs like Galatasaray, Fenerbahçe, and Beşiktaş cannot be overstated. These institutions have become synonymous with Turkish identity, often transcending the realm of sport to become focal points for community pride and social cohesion. The intense rivalries, particularly between Galatasaray and Fenerbahçe, have evolved over the decades, reflecting changes in society and the sport itself. Matches between these clubs, known as the \"Intercontinental Derby,\" are not merely contests for points but are events that galvanize the entire nation, showcasing the deep emotional ties fans have to their teams.\n","\n","The performance of the Turkish national team in international tournaments has also mirrored the growth and challenges of soccer in Turkey. Achievements such as reaching the semifinals of the UEFA European Championship in 2008 highlighted the potential and talent within Turkish soccer, bringing the sport to the forefront of national consciousness. However, the national team has faced struggles, including failures to qualify for major tournaments and issues arising from governance, such as match-fixing scandals that have marred the integrity of the sport.\n","\n","Despite these challenges, Turkish soccer maintains a robust presence in the global football landscape. The Süper Lig has produced numerous talented players who have gone on to achieve international acclaim, and the passionate fan base that supports local clubs continues to drive the sport’s popularity. Turkish soccer remains a significant aspect of the nation's culture, symbolizing hope, unity, and resilience amidst the complexities of modern society. As Turkey continues to navigate both domestic and international football landscapes, the rich history of soccer in the country serves as a testament to its enduring legacy and potential for future success.\n"]}]},{"cell_type":"markdown","source":["## 4. Reflection (iterative)"],"metadata":{"id":"7UdwXMo5ITmA"}},{"cell_type":"code","source":["import os\n","from dotenv import load_dotenv\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.messages import SystemMessage, HumanMessage\n","\n","\n","os.environ['OPENAI_API_KEY'] = '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'\n","\n","\n","# Check if the API key is set\n","if not os.getenv(\"OPENAI_API_KEY\"):\n","    raise ValueError(\"OPENAI_API_KEY not found in .env file. Please add it.\")\n","\n","# Initialize the Chat LLM. We use a powerful model like gpt-4o for better reasoning.\n","# A lower temperature is used for more deterministic and focused outputs.\n","llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.1)\n","\n","\n","def run_reflection_loop():\n","    \"\"\"\n","    Demonstrates a multi-step AI reflection loop to progressively improve a Python function.\n","    \"\"\"\n","\n","    # --- The Core Task ---\n","    task_prompt = \"\"\"\n","    Your task is to create a Python function named `calculate_factorial`.\n","    This function should do the following:\n","    1.  Accept a single integer `n` as input.\n","    2.  Calculate its factorial (n!).\n","    3.  Include a clear docstring explaining what the function does.\n","    4.  Handle edge cases: The factorial of 0 is 1.\n","    5.  Handle invalid input: Raise a ValueError if the input is a negative number.\n","    \"\"\"\n","\n","    # --- The Reflection Loop ---\n","    max_iterations = 3\n","    current_code = \"\"\n","    # We will build a conversation history to provide context in each step.\n","    message_history = [HumanMessage(content=task_prompt)]\n","\n","    for i in range(max_iterations):\n","        print(\"\\n\" + \"=\"*25 + f\" REFLECTION LOOP: ITERATION {i + 1} \" + \"=\"*25)\n","\n","        # --- 1. GENERATE / REFINE STAGE ---\n","        # In the first iteration, it generates. In subsequent iterations, it refines.\n","        if i == 0:\n","            print(\"\\n>>> STAGE 1: GENERATING initial code...\")\n","            # The first message is just the task prompt.\n","            response = llm.invoke(message_history)\n","            current_code = response.content\n","        else:\n","            print(\"\\n>>> STAGE 1: REFINING code based on previous critique...\")\n","            # The message history now contains the task, the last code, and the last critique.\n","            # We instruct the model to apply the critiques.\n","            message_history.append(HumanMessage(content=\"Please refine the code using the critiques provided.\"))\n","            response = llm.invoke(message_history)\n","            current_code = response.content\n","\n","        print(\"\\n--- Generated Code (v\" + str(i + 1) + \") ---\\n\" + current_code)\n","        message_history.append(response) # Add the generated code to history\n","\n","        # --- 2. REFLECT STAGE ---\n","        print(\"\\n>>> STAGE 2: REFLECTING on the generated code...\")\n","\n","        # Create a specific prompt for the reflector agent.\n","        # This asks the model to act as a senior code reviewer.\n","        reflector_prompt = [\n","            SystemMessage(content=\"\"\"\n","                You are a senior software engineer and an expert in Python.\n","                Your role is to perform a meticulous code review.\n","                Critically evaluate the provided Python code based on the original task requirements.\n","                Look for bugs, style issues, missing edge cases, and areas for improvement.\n","                If the code is perfect and meets all requirements, respond with the single phrase 'CODE_IS_PERFECT'.\n","                Otherwise, provide a bulleted list of your critiques.\n","            \"\"\"),\n","            HumanMessage(content=f\"Original Task:\\n{task_prompt}\\n\\nCode to Review:\\n{current_code}\")\n","        ]\n","\n","        critique_response = llm.invoke(reflector_prompt)\n","        critique = critique_response.content\n","\n","        # --- 3. STOPPING CONDITION ---\n","        if \"CODE_IS_PERFECT\" in critique:\n","            print(\"\\n--- Critique ---\\nNo further critiques found. The code is satisfactory.\")\n","            break\n","\n","        print(\"\\n--- Critique ---\\n\" + critique)\n","        # Add the critique to the history for the next refinement loop.\n","        message_history.append(HumanMessage(content=f\"Critique of the previous code:\\n{critique}\"))\n","\n","    print(\"\\n\" + \"=\"*30 + \" FINAL RESULT \" + \"=\"*30)\n","    print(\"\\nFinal refined code after the reflection process:\\n\")\n","    print(current_code)\n","\n","\n","if __name__ == \"__main__\":\n","    run_reflection_loop()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uRlLuiDWIOla","executionInfo":{"status":"ok","timestamp":1760478868867,"user_tz":420,"elapsed":41036,"user":{"displayName":"umut ozertem","userId":"07706305058932434778"}},"outputId":"68f37191-4b15-4fe1-e050-7876bba7fd34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","========================= REFLECTION LOOP: ITERATION 1 =========================\n","\n",">>> STAGE 1: GENERATING initial code...\n","\n","--- Generated Code (v1) ---\n","Here's a Python function named `calculate_factorial` that meets the specified requirements:\n","\n","```python\n","def calculate_factorial(n):\n","    \"\"\"\n","    Calculate the factorial of a non-negative integer n.\n","\n","    The factorial of a non-negative integer n is the product of all positive integers less than or equal to n.\n","    The factorial of 0 is defined as 1.\n","\n","    Parameters:\n","    n (int): A non-negative integer whose factorial is to be calculated.\n","\n","    Returns:\n","    int: The factorial of the input integer n.\n","\n","    Raises:\n","    ValueError: If the input is a negative integer.\n","    \"\"\"\n","    if n < 0:\n","        raise ValueError(\"Factorial is not defined for negative numbers.\")\n","    \n","    factorial = 1\n","    for i in range(2, n + 1):\n","        factorial *= i\n","    \n","    return factorial\n","\n","# Example usage:\n","# print(calculate_factorial(5))  # Output: 120\n","# print(calculate_factorial(0))  # Output: 1\n","```\n","\n","### Explanation:\n","1. **Docstring**: The function includes a docstring that explains what the function does, its parameters, return value, and the exception it raises.\n","2. **Edge Case Handling**: The function correctly handles the case where `n` is 0 by initializing `factorial` to 1, which is the factorial of 0.\n","3. **Invalid Input Handling**: The function raises a `ValueError` if the input `n` is negative, as factorials are not defined for negative numbers.\n","4. **Factorial Calculation**: The function uses a loop to calculate the factorial of `n` by multiplying all integers from 2 to `n`. This is efficient and straightforward for non-negative integers.\n","\n",">>> STAGE 2: REFLECTING on the generated code...\n","\n","--- Critique ---\n","- The code correctly handles the calculation of factorials for non-negative integers, including the edge case where `n` is 0.\n","- The docstring is clear and provides a good explanation of the function's purpose, parameters, return value, and exceptions.\n","- The function raises a `ValueError` for negative inputs, which is appropriate as factorials are not defined for negative numbers.\n","- The loop for calculating the factorial starts at 2, which is efficient since multiplying by 1 is unnecessary.\n","- The code could be improved by adding a type check to ensure that the input `n` is an integer. This would prevent potential issues if a non-integer type is passed to the function.\n","- Consider adding type hints to the function signature for better clarity and to leverage static type checking tools.\n","\n","Here is the revised code with these improvements:\n","\n","```python\n","def calculate_factorial(n: int) -> int:\n","    \"\"\"\n","    Calculate the factorial of a non-negative integer n.\n","\n","    The factorial of a non-negative integer n is the product of all positive integers less than or equal to n.\n","    The factorial of 0 is defined as 1.\n","\n","    Parameters:\n","    n (int): A non-negative integer whose factorial is to be calculated.\n","\n","    Returns:\n","    int: The factorial of the input integer n.\n","\n","    Raises:\n","    ValueError: If the input is a negative integer.\n","    TypeError: If the input is not an integer.\n","    \"\"\"\n","    if not isinstance(n, int):\n","        raise TypeError(\"Input must be an integer.\")\n","    if n < 0:\n","        raise ValueError(\"Factorial is not defined for negative numbers.\")\n","    \n","    factorial = 1\n","    for i in range(2, n + 1):\n","        factorial *= i\n","    \n","    return factorial\n","\n","# Example usage:\n","# print(calculate_factorial(5))  # Output: 120\n","# print(calculate_factorial(0))  # Output: 1\n","```\n","\n","These changes ensure that the function is robust against incorrect input types and provide additional clarity through type hints.\n","\n","========================= REFLECTION LOOP: ITERATION 2 =========================\n","\n",">>> STAGE 1: REFINING code based on previous critique...\n","\n","--- Generated Code (v2) ---\n","Certainly! Here's the refined version of the `calculate_factorial` function, incorporating the critiques and improvements:\n","\n","```python\n","def calculate_factorial(n: int) -> int:\n","    \"\"\"\n","    Calculate the factorial of a non-negative integer n.\n","\n","    The factorial of a non-negative integer n is the product of all positive integers less than or equal to n.\n","    The factorial of 0 is defined as 1.\n","\n","    Parameters:\n","    n (int): A non-negative integer whose factorial is to be calculated.\n","\n","    Returns:\n","    int: The factorial of the input integer n.\n","\n","    Raises:\n","    ValueError: If the input is a negative integer.\n","    TypeError: If the input is not an integer.\n","    \"\"\"\n","    if not isinstance(n, int):\n","        raise TypeError(\"Input must be an integer.\")\n","    if n < 0:\n","        raise ValueError(\"Factorial is not defined for negative numbers.\")\n","    \n","    factorial = 1\n","    for i in range(2, n + 1):\n","        factorial *= i\n","    \n","    return factorial\n","\n","# Example usage:\n","# print(calculate_factorial(5))  # Output: 120\n","# print(calculate_factorial(0))  # Output: 1\n","```\n","\n","### Key Improvements:\n","1. **Type Checking**: Added a check to ensure that the input `n` is an integer, raising a `TypeError` if it is not. This prevents issues with non-integer inputs.\n","2. **Type Hints**: Added type hints to the function signature (`n: int` and `-> int`) to clarify the expected input and output types, which can be useful for static type checking tools and improving code readability.\n","\n","These refinements make the function more robust and clear, ensuring it handles a wider range of potential input issues gracefully.\n","\n",">>> STAGE 2: REFLECTING on the generated code...\n","\n","--- Critique ---\n","- The code correctly handles the task requirements, including calculating the factorial, handling the edge case for 0, and raising a `ValueError` for negative numbers.\n","- The addition of a `TypeError` for non-integer inputs is a good enhancement, although it was not explicitly required by the original task.\n","- The docstring is clear and provides a good explanation of the function's purpose, parameters, return value, and exceptions.\n","- The use of type hints improves code readability and assists with static type checking.\n","- The loop correctly calculates the factorial by iterating from 2 to `n`, which is efficient and clear.\n","\n","Overall, the code is well-written and meets the task requirements, with additional improvements for robustness and clarity.\n","\n","========================= REFLECTION LOOP: ITERATION 3 =========================\n","\n",">>> STAGE 1: REFINING code based on previous critique...\n","\n","--- Generated Code (v3) ---\n","The critiques provided are generally positive and affirm that the code meets the task requirements effectively. However, since the feedback suggests that the addition of a `TypeError` was not explicitly required by the original task, we can consider this an optional enhancement rather than a necessity. Nonetheless, including it improves robustness, so I'll keep it in the refined version. Here's the code with a slight refinement to ensure clarity and maintain the enhancements:\n","\n","```python\n","def calculate_factorial(n: int) -> int:\n","    \"\"\"\n","    Calculate the factorial of a non-negative integer n.\n","\n","    The factorial of a non-negative integer n is the product of all positive integers less than or equal to n.\n","    The factorial of 0 is defined as 1.\n","\n","    Parameters:\n","    n (int): A non-negative integer whose factorial is to be calculated.\n","\n","    Returns:\n","    int: The factorial of the input integer n.\n","\n","    Raises:\n","    ValueError: If the input is a negative integer.\n","    TypeError: If the input is not an integer.\n","    \"\"\"\n","    if not isinstance(n, int):\n","        raise TypeError(\"Input must be an integer.\")\n","    if n < 0:\n","        raise ValueError(\"Factorial is not defined for negative numbers.\")\n","    \n","    factorial = 1\n","    for i in range(2, n + 1):\n","        factorial *= i\n","    \n","    return factorial\n","\n","# Example usage:\n","# print(calculate_factorial(5))  # Output: 120\n","# print(calculate_factorial(0))  # Output: 1\n","```\n","\n","### Key Points:\n","- **Type Checking**: The `TypeError` for non-integer inputs remains, as it enhances the function's robustness.\n","- **Type Hints**: These are retained for clarity and to assist with static type checking.\n","- **Docstring**: The docstring remains clear and informative, covering the function's purpose, parameters, return value, and exceptions.\n","\n","This version of the code maintains the enhancements and clarity while adhering to the original task requirements.\n","\n",">>> STAGE 2: REFLECTING on the generated code...\n","\n","--- Critique ---\n","- The code correctly implements the task requirements, including handling edge cases and invalid inputs.\n","- The `TypeError` for non-integer inputs is a good addition for robustness, even though it wasn't explicitly required by the task.\n","- The docstring is clear and informative, explaining the function's purpose, parameters, return value, and exceptions.\n","- Type hints are used appropriately, aiding in code clarity and static type checking.\n","- The loop for calculating the factorial is efficient and correctly starts from 2, as multiplying by 1 is unnecessary.\n","- Example usage comments are helpful for understanding how to use the function.\n","\n","Overall, the code is well-written and meets the task requirements effectively.\n","\n","============================== FINAL RESULT ==============================\n","\n","Final refined code after the reflection process:\n","\n","The critiques provided are generally positive and affirm that the code meets the task requirements effectively. However, since the feedback suggests that the addition of a `TypeError` was not explicitly required by the original task, we can consider this an optional enhancement rather than a necessity. Nonetheless, including it improves robustness, so I'll keep it in the refined version. Here's the code with a slight refinement to ensure clarity and maintain the enhancements:\n","\n","```python\n","def calculate_factorial(n: int) -> int:\n","    \"\"\"\n","    Calculate the factorial of a non-negative integer n.\n","\n","    The factorial of a non-negative integer n is the product of all positive integers less than or equal to n.\n","    The factorial of 0 is defined as 1.\n","\n","    Parameters:\n","    n (int): A non-negative integer whose factorial is to be calculated.\n","\n","    Returns:\n","    int: The factorial of the input integer n.\n","\n","    Raises:\n","    ValueError: If the input is a negative integer.\n","    TypeError: If the input is not an integer.\n","    \"\"\"\n","    if not isinstance(n, int):\n","        raise TypeError(\"Input must be an integer.\")\n","    if n < 0:\n","        raise ValueError(\"Factorial is not defined for negative numbers.\")\n","    \n","    factorial = 1\n","    for i in range(2, n + 1):\n","        factorial *= i\n","    \n","    return factorial\n","\n","# Example usage:\n","# print(calculate_factorial(5))  # Output: 120\n","# print(calculate_factorial(0))  # Output: 1\n","```\n","\n","### Key Points:\n","- **Type Checking**: The `TypeError` for non-integer inputs remains, as it enhances the function's robustness.\n","- **Type Hints**: These are retained for clarity and to assist with static type checking.\n","- **Docstring**: The docstring remains clear and informative, covering the function's purpose, parameters, return value, and exceptions.\n","\n","This version of the code maintains the enhancements and clarity while adhering to the original task requirements.\n"]}]},{"cell_type":"markdown","source":["##5. reflection - langchain"],"metadata":{"id":"5YhpH6gOMTBW"}},{"cell_type":"code","source":["import os\n","import sys\n","import asyncio\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","\n","import nest_asyncio\n","nest_asyncio.apply()\n","\n","# --- Configuration ---\n","# Ensure your API key environment variable is set (e.g., OPENAI_API_KEY)\n","try:\n","    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n","    print(f\"Language model initialized: {llm.model_name}\")\n","except Exception as e:\n","    print(f\"Error initializing language model: {e}\", file=sys.stderr)\n","    print(\"Please ensure your OPENAI_API_KEY is set correctly.\", file=sys.stderr)\n","    sys.exit(1) # Exit if the LLM cannot be initialized\n","\n","\n","# --- Define Chain Components ---\n","\n","# 1. Initial Generation: Creates the first draft of the product description.\n","# The input to this chain will be a dictionary, so we update the prompt template.\n","generation_chain = (\n","    ChatPromptTemplate.from_messages([\n","        (\"system\", \"Write a short, simple product description for a new smart coffee mug.\"),\n","        (\"user\", \"{product_details}\")\n","    ])\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","# 2. Critique: Evaluates the generated description and provides feedback.\n","critique_chain = (\n","    ChatPromptTemplate.from_messages([\n","        (\"system\", \"\"\"Critique the following product description based on clarity, conciseness, and appeal.\n","        Provide specific suggestions for improvement.\"\"\"),\n","        # This will receive 'initial_description' from the previous step.\n","        (\"user\", \"Product Description to Critique:\\n{initial_description}\")\n","    ])\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","# 3. Refinement: Rewrites the description based on the original details and the critique.\n","refinement_chain = (\n","    ChatPromptTemplate.from_messages([\n","        (\"system\", \"\"\"Based on the original product details and the following critique,\n","        rewrite the product description to be more effective.\n","\n","        Original Product Details: {product_details}\n","        Critique: {critique}\n","\n","        Refined Product Description:\"\"\"),\n","        (\"user\", \"\") # User input is empty as the context is provided in the system message\n","    ])\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","\n","# --- Build the Full Reflection Chain (Refactored) ---\n","# This chain is structured to be more readable and linear.\n","full_reflection_chain = (\n","    RunnablePassthrough.assign(\n","        initial_description=generation_chain\n","    )\n","    | RunnablePassthrough.assign(\n","        critique=critique_chain\n","    )\n","    | refinement_chain\n",")\n","\n","\n","# --- Run the Chain ---\n","async def run_reflection_example(product_details: str):\n","    \"\"\"Runs the LangChain reflection example with product details.\"\"\"\n","    print(f\"\\n--- Running Reflection Example for Product: '{product_details}' ---\")\n","    try:\n","        # The chain now expects a dictionary as input from the start.\n","        final_refined_description = await full_reflection_chain.ainvoke(\n","            {\"product_details\": product_details}\n","        )\n","        print(\"\\n--- Final Refined Product Description ---\")\n","        print(final_refined_description)\n","    except Exception as e:\n","        print(f\"\\nAn error occurred during chain execution: {e}\")\n","\n","if __name__ == \"__main__\":\n","    test_product_details = \"A mug that keeps coffee hot and can be controlled by a smartphone app.\"\n","    asyncio.run(run_reflection_example(test_product_details))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yC4m8vjIMYHl","executionInfo":{"status":"ok","timestamp":1760478918128,"user_tz":420,"elapsed":24509,"user":{"displayName":"umut ozertem","userId":"07706305058932434778"}},"outputId":"1a41ea72-e4cb-4b50-e293-61a819f45898"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Language model initialized: gpt-4o-mini\n","\n","--- Running Reflection Example for Product: 'A mug that keeps coffee hot and can be controlled by a smartphone app.' ---\n","\n","--- Final Refined Product Description ---\n","**Introducing the Smart Coffee Mug**\n","\n","**Perfect Temperature Control**: Enjoy your coffee at the ideal temperature for maximum satisfaction. No more lukewarm sips—experience every cup just as you like it!\n","\n","**Advanced Smart Features**: Take charge of your coffee experience with customizable heat settings, timers, and instant alerts, all accessible through our intuitive smartphone app. \n","\n","**Sleek and Stylish Design**: This modern mug fits perfectly into your lifestyle, whether you’re at home, in the office, or on the go. Its elegant design ensures you enjoy your beverage in style.\n","\n","**Why Choose the Smart Coffee Mug?**: With the ability to keep your favorite brew at the perfect temperature, you're guaranteed every sip will be enjoyable. Say goodbye to wasted coffee and hello to a seamless, convenient coffee experience.\n","\n","**Elevate Your Coffee Ritual**: Don't settle for less—transform how you enjoy coffee today with the Smart Coffee Mug. Perfect temperature, smart convenience, and stylish design await you!\n"]}]},{"cell_type":"markdown","source":["## 6. Tool Use"],"metadata":{"id":"6oHq3HOgN4Tp"}},{"cell_type":"code","source":["import os, getpass\n","import asyncio\n","import nest_asyncio\n","from typing import List\n","from dotenv import load_dotenv\n","import logging\n","from google.adk.agents import Agent as ADKAgent, LlmAgent\n","from google.adk.runners import Runner\n","from google.adk.sessions import InMemorySessionService\n","from google.adk.tools import google_search\n","from google.adk.code_executors import BuiltInCodeExecutor\n","from google.genai import types\n","\n","import nest_asyncio\n","nest_asyncio.apply()\n","\n","\n","os.environ['GOOGLE_API_KEY'] = 'AIzaSyBjEJoyoFZ1buC2NGap1PyThe3reHK9u-o'\n","\n","\n","# Define variables required for Session setup and Agent execution\n","APP_NAME=\"calculator\"\n","USER_ID=\"user1234\"\n","SESSION_ID=\"session_code_exec_async\"\n","\n","\n","# Agent Definition\n","code_agent = LlmAgent(\n","   name=\"calculator_agent\",\n","   model=\"gemini-2.0-flash\",\n","   code_executor=BuiltInCodeExecutor(),\n","   instruction=\"\"\"You are a calculator agent.\n","   When given a mathematical expression, write and execute Python code to calculate the result.\n","   Return only the final numerical result as plain text, without markdown or code blocks.\n","   \"\"\",\n","   description=\"Executes Python code to perform calculations.\",\n",")\n","\n","# Agent Interaction (Async)\n","async def call_agent_async(query):\n","\n","   # Session and Runner\n","   session_service = InMemorySessionService()\n","   session = await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n","   runner = Runner(agent=code_agent, app_name=APP_NAME, session_service=session_service)\n","\n","   content = types.Content(role='user', parts=[types.Part(text=query)])\n","   print(f\"\\n--- Running Query: {query} ---\")\n","   final_response_text = \"No final text response captured.\"\n","   try:\n","       # Use run_async\n","       async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n","           print(f\"Event ID: {event.id}, Author: {event.author}\")\n","\n","           # --- Check for specific parts FIRST ---\n","           # has_specific_part = False\n","           if event.content and event.content.parts and event.is_final_response():\n","               for part in event.content.parts: # Iterate through all parts\n","                   if part.executable_code:\n","                       # Access the actual code string via .code\n","                       print(f\"  Debug: Agent generated code:\\n```python\\n{part.executable_code.code}\\n```\")\n","                       has_specific_part = True\n","                   elif part.code_execution_result:\n","                       # Access outcome and output correctly\n","                       print(f\"  Debug: Code Execution Result: {part.code_execution_result.outcome} - Output:\\n{part.code_execution_result.output}\")\n","                       has_specific_part = True\n","                   # Also print any text parts found in any event for debugging\n","                   elif part.text and not part.text.isspace():\n","                       print(f\"  Text: '{part.text.strip()}'\")\n","                       # Do not set has_specific_part=True here, as we want the final response logic below\n","\n","               # --- Check for final response AFTER specific parts ---\n","               text_parts = [part.text for part in event.content.parts if part.text]\n","               final_result = \"\".join(text_parts)\n","               print(f\"==> Final Agent Response: {final_result}\")\n","\n","   except Exception as e:\n","       print(f\"ERROR during agent run: {e}\")\n","   print(\"-\" * 30)\n","\n","# Main async function to run the examples\n","async def main():\n","   await call_agent_async(\"Calculate the value of (5 + 7) * 3\")\n","   await call_agent_async(\"What is 10 factorial?\")\n","\n","\n","# Execute the main async function\n","try:\n","   nest_asyncio.apply()\n","   asyncio.run(main())\n","except RuntimeError as e:\n","   # Handle specific error when running asyncio.run in an already running loop (like Jupyter/Colab)\n","   if \"cannot be called from a running event loop\" in str(e):\n","       print(\"\\nRunning in an existing event loop (like Colab/Jupyter).\")\n","       print(\"Please run `await main()` in a notebook cell instead.\")\n","       # If in an interactive environment like a notebook, you might need to run:\n","       # await main()\n","   else:\n","       raise e # Re-raise other runtime errors"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-dtia8FgN3z4","executionInfo":{"status":"ok","timestamp":1760478970482,"user_tz":420,"elapsed":23982,"user":{"displayName":"umut ozertem","userId":"07706305058932434778"}},"outputId":"b6ae5dbd-e583-4eb0-b669-6ba932189524"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Running Query: Calculate the value of (5 + 7) * 3 ---\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:google_genai.types:Warning: there are non-text parts in the response: ['executable_code', 'code_execution_result'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"]},{"output_type":"stream","name":"stdout","text":["Event ID: c8c4c593-a4b0-4283-b94a-1aec6b56eabb, Author: calculator_agent\n","  Debug: Agent generated code:\n","```python\n","print((5 + 7) * 3)\n","\n","```\n","  Debug: Code Execution Result: Outcome.OUTCOME_OK - Output:\n","36\n","\n","  Text: '36'\n","==> Final Agent Response: 36\n","\n","------------------------------\n","\n","--- Running Query: What is 10 factorial? ---\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:google_genai.types:Warning: there are non-text parts in the response: ['executable_code', 'code_execution_result'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"]},{"output_type":"stream","name":"stdout","text":["Event ID: 0046a6a0-1849-4b07-9b8f-3d8598ff2d8f, Author: calculator_agent\n","  Debug: Agent generated code:\n","```python\n","import math\n","print(math.factorial(10))\n","\n","```\n","  Debug: Code Execution Result: Outcome.OUTCOME_OK - Output:\n","3628800\n","\n","  Text: '3628800'\n","==> Final Agent Response: 3628800\n","\n","------------------------------\n"]}]},{"cell_type":"markdown","source":["## 7. Tool use toy example"],"metadata":{"id":"PD_W966lupF_"}},{"cell_type":"code","source":["import os\n","import asyncio\n","from typing import List\n","\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.tools import tool\n","from langchain.agents import create_tool_calling_agent, AgentExecutor\n","\n","os.environ['GOOGLE_API_KEY'] = 'AIzaSyBjEJoyoFZ1buC2NGap1PyThe3reHK9u-o'\n","\n","# --- Configuration ---\n","# Ensure your GOOGLE_API_KEY environment variable is set.\n","try:\n","    # A model with function/tool calling capabilities is required.\n","    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n","    print(f\"✅ Language model initialized: {llm.model_name}\")\n","except Exception as e:\n","    print(f\"🛑 Error initializing language model: {e}\")\n","    llm = None\n","\n","\n","# --- Define a Tool ---\n","@tool\n","def search_information(query: str) -> str:\n","    \"\"\"\n","    Provides factual information on a given topic. Use this tool to find answers to questions\n","    like 'What is the capital of France?' or 'What is the weather in London?'.\n","    \"\"\"\n","    print(f\"\\n--- 🛠️ Tool Called: search_information with query: '{query}' ---\")\n","    # Simulate a search tool with a dictionary of predefined results.\n","    simulated_results = {\n","        \"weather in london\": \"The weather in London is currently cloudy with a temperature of 15°C.\",\n","        \"capital of france\": \"The capital of France is Paris.\",\n","        \"population of earth\": \"The estimated population of Earth is around 8 billion people.\",\n","        \"tallest mountain\": \"Mount Everest is the tallest mountain above sea level.\",\n","        \"default\": f\"Simulated search result for '{query}': No specific information found, but the topic seems interesting.\"\n","    }\n","    result = simulated_results.get(query.lower(), simulated_results[\"default\"])\n","    print(f\"--- TOOL RESULT: {result} ---\")\n","    return result\n","\n","tools = [search_information]\n","\n","\n","# --- Create a Tool-Calling Agent ---\n","if llm:\n","    # This prompt template requires an `agent_scratchpad` placeholder for the agent's internal steps.\n","    agent_prompt = ChatPromptTemplate.from_messages([\n","        (\"system\", \"You are a helpful assistant.\"),\n","        (\"human\", \"{input}\"),\n","        (\"placeholder\", \"{agent_scratchpad}\"),\n","    ])\n","\n","    # Create the agent, binding the LLM, tools, and prompt together.\n","    agent = create_tool_calling_agent(llm, tools, agent_prompt)\n","\n","    # AgentExecutor is the runtime that invokes the agent and executes the chosen tools.\n","    # The 'tools' argument is not needed here as they are already bound to the agent.\n","    agent_executor = AgentExecutor(agent=agent, verbose=True)\n","\n","\n","    async def run_agent_with_tool(query: str):\n","        \"\"\"Invokes the agent executor with a query and prints the final response.\"\"\"\n","        print(f\"\\n--- 🏃 Running Agent with Query: '{query}' ---\")\n","        try:\n","            response = await agent_executor.ainvoke({\"input\": query})\n","            print(\"\\n--- ✅ Final Agent Response ---\")\n","            print(response[\"output\"])\n","        except Exception as e:\n","            print(f\"\\n🛑 An error occurred during agent execution: {e}\")\n","\n","    async def main():\n","        \"\"\"Runs all agent queries concurrently.\"\"\"\n","        tasks = [\n","            run_agent_with_tool(\"What is the capital of France?\"),\n","            run_agent_with_tool(\"What's the weather like in London?\"),\n","            run_agent_with_tool(\"Tell me something about dogs.\") # Should trigger the default tool response\n","        ]\n","        await asyncio.gather(*tasks)\n","\n","    if __name__ == \"__main__\":\n","        # Run all async tasks in a single event loop.\n","        asyncio.run(main())\n","\n","else:\n","    print(\"\\nSkipping agent execution due to LLM initialization failure.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_RmXLP-JTdhi","executionInfo":{"status":"ok","timestamp":1760479034815,"user_tz":420,"elapsed":647,"user":{"displayName":"umut ozertem","userId":"07706305058932434778"}},"outputId":"64a41285-82b8-48c0-e8d6-307970912c9c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:asyncio:Unclosed client session\n","client_session: <aiohttp.client.ClientSession object at 0x78ba140b2270>\n"]},{"output_type":"stream","name":"stdout","text":["🛑 Error initializing language model: 'ChatGoogleGenerativeAI' object has no attribute 'model_name'\n","\n","Skipping agent execution due to LLM initialization failure.\n"]}]},{"cell_type":"markdown","source":["##8. memory management"],"metadata":{"id":"n2LDKEE2-a5l"}},{"cell_type":"code","source":["from langchain.memory import ChatMessageHistory\n","\n","# Initialize the history object\n","history = ChatMessageHistory()\n","\n","# Add user and AI messages\n","history.add_user_message(\"I'm heading to New York next week.\")\n","history.add_ai_message(\"Great! It's a fantastic city.\")\n","\n","# Access the list of messages\n","print(history.messages)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9YTuBRAa-nmB","executionInfo":{"status":"ok","timestamp":1757817594916,"user_tz":420,"elapsed":3865,"user":{"displayName":"Cagri Ozertem","userId":"00286680361848332070"}},"outputId":"5d6bc040-1845-48ec-8e10-fcc9e6a2b2df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[HumanMessage(content=\"I'm heading to New York next week.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Great! It's a fantastic city.\", additional_kwargs={}, response_metadata={})]\n"]}]},{"cell_type":"code","source":["from langchain.memory import ConversationBufferMemory\n","\n","# Initialize memory\n","memory = ConversationBufferMemory()\n","\n","# Save a conversation turn\n","memory.save_context({\"input\": \"What's the weather like?\"}, {\"output\": \"It's sunny today.\"})\n","\n","# Load the memory as a string\n","print(memory.load_memory_variables({}))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hNPGjyKH_jsC","executionInfo":{"status":"ok","timestamp":1757817598023,"user_tz":420,"elapsed":10,"user":{"displayName":"Cagri Ozertem","userId":"00286680361848332070"}},"outputId":"7cb59d78-b8fb-4f99-b352-bebd4dbf6ef8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'history': \"Human: What's the weather like?\\nAI: It's sunny today.\"}\n"]}]},{"cell_type":"code","source":["import os\n","from langchain_openai import OpenAI\n","from langchain.chains import LLMChain\n","from langchain.prompts import PromptTemplate\n","from langchain.memory import ConversationBufferMemory\n","\n","os.environ['OPENAI_API_KEY'] = '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'\n","\n","# 1. Define LLM and Prompt\n","llm = OpenAI(temperature=0)\n","template = \"\"\"You are a helpful travel agent.\n","\n","Previous conversation:\n","{history}\n","\n","New question: {question}\n","Response:\"\"\"\n","prompt = PromptTemplate.from_template(template)\n","\n","# 2. Configure Memory\n","# The memory_key \"history\" matches the variable in the prompt\n","memory = ConversationBufferMemory(memory_key=\"history\")\n","\n","# 3. Build the Chain\n","conversation = LLMChain(llm=llm, prompt=prompt, memory=memory)\n","\n","# 4. Run the Conversation\n","response = conversation.predict(question=\"I want to book a flight.\")\n","print(response)\n","response = conversation.predict(question=\"My name is Sam, by the way.\")\n","print(response)\n","response = conversation.predict(question=\"What was my name again?\")\n","print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hqeV2fZ1AGZX","executionInfo":{"status":"ok","timestamp":1757817604501,"user_tz":420,"elapsed":4394,"user":{"displayName":"Cagri Ozertem","userId":"00286680361848332070"}},"outputId":"a199a193-7d35-4dee-8aef-971a13ad9f39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2344740022.py:25: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n","  conversation = LLMChain(llm=llm, prompt=prompt, memory=memory)\n"]},{"output_type":"stream","name":"stdout","text":[" Great! Where are you looking to travel to and when? I can help you find the best flight options and prices.\n"," Nice to meet you, Sam! Is there a specific destination or date you have in mind for your trip? I can assist you in finding the perfect flight for your needs.\n"," Your name is Sam.\n"]}]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain.chains import LLMChain\n","from langchain.memory import ConversationBufferMemory\n","from langchain_core.prompts import (\n","    ChatPromptTemplate,\n","    MessagesPlaceholder,\n","    SystemMessagePromptTemplate,\n","    HumanMessagePromptTemplate,\n",")\n","\n","# 1. Define Chat Model and Prompt\n","llm = ChatOpenAI()\n","prompt = ChatPromptTemplate(\n","    messages=[\n","        SystemMessagePromptTemplate.from_template(\"You are a friendly assistant.\"),\n","        MessagesPlaceholder(variable_name=\"chat_history\"),\n","        HumanMessagePromptTemplate.from_template(\"{question}\")\n","    ]\n",")\n","\n","# 2. Configure Memory\n","# return_messages=True is essential for chat models\n","memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n","\n","# 3. Build the Chain\n","conversation = LLMChain(llm=llm, prompt=prompt, memory=memory)\n","\n","# 4. Run the Conversation\n","response = conversation.predict(question=\"Hi, I'm Jane.\")\n","print(response)\n","response = conversation.predict(question=\"Do you remember my name?\")\n","print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZZP96T3kA48x","executionInfo":{"status":"ok","timestamp":1757817608726,"user_tz":420,"elapsed":1282,"user":{"displayName":"Cagri Ozertem","userId":"00286680361848332070"}},"outputId":"4c1f4d85-2377-40d6-d914-dbe9391149e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello Jane! It's nice to meet you. How can I assist you today?\n","Yes, I remember your name, Jane. How can I help you today?\n"]}]},{"cell_type":"markdown","source":["## 14. RAG\n"],"metadata":{"id":"Jvf7kzdFB_ba"}},{"cell_type":"code","source":["!pip install weaviate-client -V 3.26\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_m475VOE9X7","executionInfo":{"status":"ok","timestamp":1758002233431,"user_tz":420,"elapsed":6267,"user":{"displayName":"Cagri Ozertem","userId":"00286680361848332070"}},"outputId":"61e44e0b-c4b6-428d-82a6-f45e9eef42c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: weaviate-client in /usr/local/lib/python3.12/dist-packages (4.16.10)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement 3.26 (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for 3.26\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import os\n","import requests\n","from typing import List, Dict, Any, TypedDict\n","from langchain_community.document_loaders import TextLoader\n","\n","from langchain_core.documents import Document\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_community.embeddings import OpenAIEmbeddings\n","from langchain_community.vectorstores import Weaviate\n","from langchain_openai import ChatOpenAI\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.schema.runnable import RunnablePassthrough\n","from langgraph.graph import StateGraph, END\n","import weaviate\n","from weaviate.embedded import EmbeddedOptions\n","import dotenv\n","\n","# Load environment variables (e.g., OPENAI_API_KEY)\n","dotenv.load_dotenv()\n","# Set your OpenAI API key (ensure it's loaded from .env or set here)\n","# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n","\n","# --- 1. Data Preparation (Preprocessing) ---\n","# Load data\n","url = \"https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/state_of_the_union.txt\"\n","res = requests.get(url)\n","\n","with open(\"state_of_the_union.txt\", \"w\") as f:\n","    f.write(res.text)\n","\n","loader = TextLoader('./state_of_the_union.txt')\n","documents = loader.load()\n","\n","# Chunk documents\n","text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n","chunks = text_splitter.split_documents(documents)\n","\n","# Embed and store chunks in Weaviate\n","client = weaviate.Client(\n","    embedded_options = EmbeddedOptions()\n",")\n","\n","vectorstore = Weaviate.from_documents(\n","    client = client,\n","    documents = chunks,\n","    embedding = OpenAIEmbeddings(),\n","    by_text = False\n",")\n","\n","# Define the retriever\n","retriever = vectorstore.as_retriever()\n","\n","# Initialize LLM\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","# --- 2. Define the State for LangGraph ---\n","class RAGGraphState(TypedDict):\n","    question: str\n","    documents: List[Document]\n","    generation: str\n","\n","# --- 3. Define the Nodes (Functions) ---\n","\n","def retrieve_documents_node(state: RAGGraphState) -> RAGGraphState:\n","    \"\"\"Retrieves documents based on the user's question.\"\"\"\n","    question = state[\"question\"]\n","    documents = retriever.invoke(question)\n","    return {\"documents\": documents, \"question\": question, \"generation\": \"\"}\n","\n","def generate_response_node(state: RAGGraphState) -> RAGGraphState:\n","    \"\"\"Generates a response using the LLM based on retrieved documents.\"\"\"\n","    question = state[\"question\"]\n","    documents = state[\"documents\"]\n","\n","    # Prompt template from the PDF\n","    template = \"\"\"You are an assistant for question-answering tasks.\n","Use the following pieces of retrieved context to answer the question.\n","If you don't know the answer, just say that you don't know.\n","Use three sentences maximum and keep the answer concise.\n","Question: {question}\n","Context: {context}\n","Answer:\n","\"\"\"\n","    prompt = ChatPromptTemplate.from_template(template)\n","\n","    # Format the context from the documents\n","    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n","\n","    # Create the RAG chain\n","    rag_chain = prompt | llm | StrOutputParser()\n","\n","    # Invoke the chain\n","    generation = rag_chain.invoke({\"context\": context, \"question\": question})\n","    return {\"question\": question, \"documents\": documents, \"generation\": generation}\n","\n","# --- 4. Build the LangGraph Graph ---\n","\n","workflow = StateGraph(RAGGraphState)\n","\n","# Add nodes\n","workflow.add_node(\"retrieve\", retrieve_documents_node)\n","workflow.add_node(\"generate\", generate_response_node)\n","\n","# Set the entry point\n","workflow.set_entry_point(\"retrieve\")\n","\n","# Add edges (transitions)\n","workflow.add_edge(\"retrieve\", \"generate\")\n","workflow.add_edge(\"generate\", END)\n","\n","# Compile the graph\n","app = workflow.compile()\n","\n","# --- 5. Run the RAG Application ---\n","if __name__ == \"__main__\":\n","    print(\"\\n--- Running RAG Query ---\")\n","    query = \"What did the president say about Justice Breyer\"\n","    inputs = {\"question\": query}\n","    for s in app.stream(inputs):\n","        print(s)\n","\n","    print(\"\\n--- Running another RAG Query ---\")\n","    query_2 = \"What did the president say about the economy?\"\n","    inputs_2 = {\"question\": query_2}\n","    for s in app.stream(inputs_2):\n","        print(s)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"W8F-tlpMCE1Y","executionInfo":{"status":"error","timestamp":1758002029555,"user_tz":420,"elapsed":1451,"user":{"displayName":"Cagri Ozertem","userId":"00286680361848332070"}},"outputId":"b714014a-be36-4f8e-a214-da3539818440"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:langchain_text_splitters.base:Created a chunk of size 3173, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 822, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 15897, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 755, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 621, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1537, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 563, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 610, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 598, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 600, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 778, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1281, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 2550, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1145, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 2396, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1082, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1295, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1323, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1119, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1422, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 941, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1361, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1451, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1234, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 515, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 625, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 625, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 625, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 545, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 872, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 805, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 529, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 519, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 523, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 860, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 805, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 584, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 625, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 625, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 807, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 586, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 611, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 513, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 806, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1470, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1432, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 2366, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1436, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 590, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 2591, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 4318, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 533, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 864, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 521, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 580, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 596, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 662, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 647, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 597, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1664, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1566, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 697, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 660, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 856, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 827, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 546, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 841, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1156, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1210, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 922, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 802, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 898, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 899, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 630, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 537, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 952, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 750, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 650, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1739, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 3030, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1828, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 532, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 724, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 625, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1265, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1208, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1367, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1172, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 975, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 765, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1317, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1747, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1369, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1837, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1595, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1260, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1552, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1756, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1288, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1417, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 2006, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1457, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1357, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 9763, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 66509, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1275, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 553, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 589, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1165, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 1047, which is longer than the specified 500\n","WARNING:langchain_text_splitters.base:Created a chunk of size 2727, which is longer than the specified 500\n","/tmp/ipython-input-1780646959.py:40: DeprecationWarning: \n","Python client v3 `weaviate.Client(...)` has been removed.\n","\n","Upgrade your code to use Python client v4 `weaviate.WeaviateClient` connections and methods.\n","    - For Python Client v4 usage, see: https://weaviate.io/developers/weaviate/client-libraries/python\n","    - For code migration, see: https://weaviate.io/developers/weaviate/client-libraries/python/v3_v4_migration\n","\n","If you have to use v3 code, install the v3 client and pin the v3 dependency in your requirements file: `weaviate-client>=3.26.7;<4.0.0`\n","  client = weaviate.Client(\n"]},{"output_type":"error","ename":"TypeError","evalue":"Client.__init__() got an unexpected keyword argument 'embedded_options'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1780646959.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Embed and store chunks in Weaviate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m client = weaviate.Client(\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0membedded_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddedOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m )\n","\u001b[0;31mTypeError\u001b[0m: Client.__init__() got an unexpected keyword argument 'embedded_options'"]}]}]}